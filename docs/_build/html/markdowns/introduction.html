<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction &mdash; PyTorchDecomp 1.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=6efca38a"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PyTorchDecomp
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">PyTorchDecomp: A set of matrix decomposition models implemented as PyTorch classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributors.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to PyTorchDecomp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_of_conduct.html">Code of conduct</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyTorchDecomp</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/markdowns/introduction.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<section id="concept-of-pytorchdecomp">
<h2>Concept of PyTorchDecomp<a class="headerlink" href="#concept-of-pytorchdecomp" title="Link to this heading"></a></h2>
<p>In data science, we sometimes deal with thousands to millions of high-dimensional data (e.g., images, omics). Usually when machine learning is performed on such high-dimensional data, the data is projected once to a lower dimension to reduce the effects of the curse of dimensionality, and then applied for downstream learning (further dimensionality reduction, data reconstruction, clustering, predictive model building, optimal transport, etc.).</p>
<p>This dimensionality reduction step is often performed independently of downstream learning as a pre-processing step (Figure 1). However, in such cases, the unsupervised dimensionality reduction is performed without being aware of subsequent learning, which may lead to the extraction of patterns from the data that are not needed for subsequent learning. In the deep machine learning field, “End-to-End” modeling is used to unify such pre-processing and downstream learning as a single model. PyTorchDecomp follows this deep machine learning technique by combining dimensionality reduction and subsequent learning based on the PyTorch framework in a End-to-End manner (Figure 1).</p>
<p><img alt="Figure 1" src="markdowns/_static/img/figure1.png" /></p>
<p>With PyTorchDecomp, an unsupervised dimensionality reduction model can be immediately converted to the supervised version. For example, consider PCA regression (PCAR), in which high-dimensional data are subjected to dimensionality reduction by PCA, followed by linear regression. In PCA as the pre-processing step, the loading matrix to project the data from a higher dimension to a lower dimension is learned to maximize the variance of the scores in the lower dimension, but maximizing variance does not always contribute to better predictive performance in subsequent regression because of outlier samples and subpopulations that do not follow labels. On the other hand, PyTorchDecomp can be used to treat dimensionality reduction and subsequent regression in an End-to-End manner. For example, in PyTorchDecomp, PCAR can be easily converted to Partial Least Squares (PLS), which can use both high-dimensional data and the corresponding label data to learn a projection matrix such that the covariance between them is maximized (!!! Link to Tutorial 3!!!) .</p>
</section>
<section id="why-matrix-decomposition-in-pytorch">
<h2>Why Matrix Decomposition in PyTorch?<a class="headerlink" href="#why-matrix-decomposition-in-pytorch" title="Link to this heading"></a></h2>
<p>Since the research communities of matrix factorization and deep learning are considered to be different, it is possible to import discussions between different research communities to each other, which could have the following advantages.</p>
<p>The advantages of using PyTorchDecomp from the point of view of users of matrix factorization algorithms may include the following:</p>
<ul class="simple">
<li><p><strong>Easy to speed up</strong>: Parameter optimization in PyTorch is based on iterational computations, such as the (stochastic) gradient method. This is often faster than conventional parameter optimization based on matrix diagonalization or inverse matrix computation that has been used in multivariate analysis field, and is easier to apply to large data sets. In addition, PyTorch supports GPU computation, which is expected to further speed up the process.</p></li>
<li><p><strong>Easy to extend the model</strong>: PyTorch optimizes parameters based on automatic differentiation. This differs from the conventional multivariate analysis approach, in which the parameter’s derivative of the objective function (gradient) is obtained once analytically and optimized, because the objective function is written directly, the user’s desired solution can be more easily expressed by merging regularization terms such as L1/L2 regularization or other model’s term.</p></li>
</ul>
<p>On the other hand, the advantages of using PyTorchDecomp from the point of view of users of deep machine learning algorithms may include the following:</p>
<ul class="simple">
<li><p><strong>Easy to interpret the result</strong>: Unlike highlly multi-layered neural network, matrix factorization can be represented as a one-layer neural network, and the parts of the data that contribute to the result can be easily identified.</p></li>
<li><p><strong>High stability of learning</strong>: Although dimensionality can be reduced even with multilayer neural networks that gradually reduce dimensions by layering activation functions, the matrix factorization algorithm has a long history and its convergence has often been well studied, making it relatively computationally stable. For example, in the task of extracting non-negative patterns from non-negative matrix data, the approach of forcing non-negative values with the abs() function after dimensional compression could be considered, but smooth optimization according to the multiplicative update (MU)-rule of NMF may yield better convergence and better results. Similarly, in the task of extracting discretized patterns of {0,1} from continuous data, it is more computationally stable to add a regularization term to make it easier to obtain binary values rather than binarizing with a threshold value to force them to be binarized.</p></li>
<li><p><strong>Reduction of model size</strong>: Compared with deep neural networks, matrix decomposition is shallow (i.e., single-layer neural networks), which means that the computation requires lower memory usage and CPU/GPU computation.</p></li>
</ul>
</section>
<section id="matrix-decomposition-algorithms-available-in-pytorchdecomp">
<h2>Matrix Decomposition algorithms available in PyTorchDecomp<a class="headerlink" href="#matrix-decomposition-algorithms-available-in-pytorchdecomp" title="Link to this heading"></a></h2>
<p>To date, the following matrix factorization algorithms have been implemented based on PyTorch’s torch.nn.Module class. This means that the following algorithms can be easily mixed with other PyTorch-based models.</p>
<ul class="simple">
<li><p><em>Unsupervised Matrix Decomposition</em></p>
<ul>
<li><p><em>Real asymmetric square matrix（<a class="reference external" href="https://chiba-ai-med.github.io/PyTorchDecomp/tutorials.html">Tutorial 1</a>）</em></p>
<ul>
<li><p><strong>LU Decomposition（<a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p></li>
<li><p><strong>QR Decomposition（<a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p></li>
</ul>
</li>
<li><p><em>Real symmetric square matrix（<a class="reference external" href="https://chiba-ai-med.github.io/PyTorchDecomp/tutorials.html">Tutorial 1</a>）</em></p>
<ul>
<li><p><strong>Cholesky Decomposition（<a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p></li>
<li><p><strong>Eigenvalue Decomposition（<a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p></li>
</ul>
</li>
<li><p><em>Real matrix（<a class="reference external" href="https://chiba-ai-med.github.io/PyTorchDecomp/tutorials.html">Tutorial 2</a>）</em></p>
<ul>
<li><p><strong>Principal Component Analysis (PCA, <a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p>
<ul>
<li><p>Rec-mode (The high dimensional data is once projected to the lower dimensional space, and then is reconstructed to the original dimension)</p></li>
<li><p>Factor-mode (The variance of the score is maximized in the lower dimensional space)</p></li>
</ul>
</li>
<li><p><strong>Independent Component Analysis（ICA, <a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p>
<ul>
<li><p>Kurtosis-based</p></li>
<li><p>Negentropy-based</p></li>
<li><p>Deep Deterministic ICA (DDICA, <a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</p></li>
</ul>
</li>
<li><p><em>Non-negative matrix（<a class="reference external" href="https://chiba-ai-med.github.io/PyTorchDecomp/tutorials.html">Tutorial 2</a>）</em></p>
<ul>
<li><p><strong>Non-negative Matrix Factorization（NMF, <a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><em>Supervised Matrix Decomposition（<a class="reference external" href="https://chiba-ai-med.github.io/PyTorchDecomp/tutorials.html">Tutorial 3</a>）</em></p>
<ul>
<li><p><strong>Partial Least Squares（PLS, <a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a>）</strong></p>
<ul>
<li><p>Rec-mode</p></li>
<li><p>Factor-mode</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img alt="Figure 2" src="markdowns/_static/img/figure2.png" /></p>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>LU/QR/Cholesky/Eigenvalue Decomposition</strong></p>
<ul>
<li><p>Gene H. Golub, Charles F. Van Loan Matrix Computations (Johns Hopkins Studies in the Mathematical Sciences)</p></li>
</ul>
</li>
<li><p><strong>Principal Component Analysis (PCA) / Partial Least Squares (PLS)</strong></p>
<ul>
<li><p>R. Arora, A. Cotter, K. Livescu and N. Srebro, Stochastic optimization for PCA and PLS, 2012 50th Annual Allerton Conference on Communication, Control, and Computing, 2012, 861-868. 2012</p></li>
</ul>
</li>
<li><p><strong>Independent Component Analysis (ICA)</strong></p>
<ul>
<li><p>Hybarinen, A. and Oja, E. Independent component analysis: algorithms and applications, Neural Networks, 13, 411-430. 2000</p></li>
</ul>
</li>
<li><p><strong>Deep Deterministic ICA (DDICA)</strong></p>
<ul>
<li><p>H. Li, S. Yu and J. C. Príncipe, Deep Deterministic Independent Component Analysis for Hyperspectral Unmixing, 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3878-3882, 2022</p></li>
</ul>
</li>
<li><p><strong>Non-negative Matrix Factorization (NMF)</strong></p>
<ul>
<li><p>Kimura, K. A Study on Efficient Algorithms for Nonnegative Matrix/Tensor Factorization, Ph.D. Thesis, 2017</p></li>
<li><p><strong>Exponent term depending on Beta parameter</strong></p>
<ul>
<li><p>Nakano, M. et al., Convergence-guaranteed multiplicative algorithms for nonnegative matrix factorization with Beta-divergence. IEEE MLSP, 283-288, 2010</p></li>
</ul>
</li>
<li><p><strong>Beta-divergence NMF and Backpropagation</strong></p>
<ul>
<li><p>https://yoyololicon.github.io/posts/2021/02/torchnmf-algorithm/</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Koki Tsuyuzaki.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>