{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a195941-8ab3-4c6a-a0c4-86e31aee794d",
   "metadata": {},
   "source": [
    "# 1. Basic Matrix Decomposition (LU, Cholesky, QR, and EVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819dd57-3f5b-4d55-965c-b01571e36c63",
   "metadata": {},
   "source": [
    "Here, we perform some matrix factorization algorithms for a real nonsymmetric square matrix and a real symmetric square matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44ff5e-4909-4049-89df-e562b4ca839b",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdc85c-938f-40ae-b5b2-c4545dd12e7c",
   "metadata": {},
   "source": [
    "First, install `PyTorchDecomp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99578e7f-fd37-4778-80a3-74196ce6d21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n",
      "CompletedProcess(args=['python', '-V'], returncode=0)\n",
      "Collecting git+https://github.com/chiba-ai-med/PyTorchDecomp.git\n",
      "  Cloning https://github.com/chiba-ai-med/PyTorchDecomp.git to /private/var/folders/72/2fkkzl210n779s1tf_59t8dr0000gn/T/pip-req-build-93a3xx2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/chiba-ai-med/PyTorchDecomp.git /private/var/folders/72/2fkkzl210n779s1tf_59t8dr0000gn/T/pip-req-build-93a3xx2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/chiba-ai-med/PyTorchDecomp.git to commit 68bd11b79660b4d88f88aba755d749169e0a54f2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /Users/koki/mambaforge/envs/pytorch/lib/python3.10/site-packages (from torchdecomp==1.3.0) (3.8.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /Users/koki/mambaforge/envs/pytorch/lib/python3.10/site-packages (from torchdecomp==1.3.0) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.2.0 in /Users/koki/mambaforge/envs/pytorch/lib/python3.10/site-packages (from torchdecomp==1.3.0) (2.3.0)\n",
      "Collecting torchvision<0.18.0,>=0.17.0 (from torchdecomp==1.3.0)\n",
      "  Downloading torchvision-0.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "INFO: pip is looking at multiple versions of torchdecomp to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Package 'torchdecomp' requires a different Python: 3.10.14 not in '<4.0,>=3.12'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'pip install git+https://github.com/chiba-ai-med/PyTorchDecomp.git' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/chiba-ai-med/PyTorchDecomp.git\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m install_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install git+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstall_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch/lib/python3.10/subprocess.py:369\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'pip install git+https://github.com/chiba-ai-med/PyTorchDecomp.git' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# Package Loading\n",
    "import subprocess\n",
    "\n",
    "# Check Python Version (>= 3.10)\n",
    "print(subprocess.run([\"python\", \"-V\"]))\n",
    "\n",
    "# Install PyTorch\n",
    "repo_url = \"https://github.com/chiba-ai-med/PyTorchDecomp.git\"\n",
    "install_command = f\"pip install git+{repo_url}\"\n",
    "subprocess.check_call(install_command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9900c61-5789-4459-931f-5fad63099805",
   "metadata": {},
   "source": [
    "## Package Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1872831-9562-48b2-afcf-aeedcbe15982",
   "metadata": {},
   "source": [
    "Then, load the necessary packages for data analysis, including `PyTorchDecomp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3938782d-ba82-4552-88f0-d26cadca70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchdecomp as td\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0c542-aec0-4db4-85ad-b624d71fcf17",
   "metadata": {},
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5257b7-4c60-45f8-95c2-67296ce4c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb87eea-266c-499a-b4e5-e7973957725f",
   "metadata": {},
   "source": [
    "## Asymmetric Square Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc767a-8bc3-443f-9bc5-6921c55f11d4",
   "metadata": {},
   "source": [
    "First, the LU and QR decompositions are performed on `PyTorch` as matrix decomposition algorithms for an asymmetric square matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39acc5-6c0a-4db0-a85c-5c18dd5c6461",
   "metadata": {},
   "source": [
    "### Toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45bef2-1e42-4194-99b2-2d7bfc8b4219",
   "metadata": {},
   "source": [
    "Prepare the following artificial toy data.\n",
    "In order to visually inspect the patterns in the data detected by each matrix decomposition algorithm, four blocks with larger values than the other elements are placed as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e3b0b-4ec5-4451-a8b0-5539173b503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123456)\n",
    "asym_mat = torch.rand(30, 30, dtype=torch.float64)\n",
    "asym_mat[5:14, 7] = 5\n",
    "asym_mat[1:4, 25:29] = 5\n",
    "asym_mat[9:19, 15:16] = 5\n",
    "asym_mat[25:29, 25:29] = 5\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolor(asym_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170ffe0-73e1-40f7-b714-56e6556de9fe",
   "metadata": {},
   "source": [
    "### GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721c099-0658-4f0d-a7e2-dfce41a0feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "asym_mat = asym_mat.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe77c80-c063-48fb-9cf7-12ee80ff6d4e",
   "metadata": {},
   "source": [
    "### LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fe3f2-1b6d-407b-9f64-0508abf73b43",
   "metadata": {},
   "source": [
    "In the LU decomposition, the asymmetric square matrix $X$ ($n \\times n$) is decomposed as the product of two factor matrices $L$ and $U$ as follows.\n",
    "\n",
    "$$\n",
    "X = LU\n",
    "$$\n",
    "\n",
    ", where $L$ is an $n \\times n$ lower triangular matrix and $U$ is an $n \\times n$ upper triangular matrix, and only $L$ has all diagonal elements 1. LU decomposition is often used as a pre-processing step to make problems easier to solve, such as solving simultaneous equations or raandomized SVD, by utilizing the properties of triangular matrices such as $L$ and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45523a-a1a4-4476-b493-1a2c9b5526a2",
   "metadata": {},
   "source": [
    "To realize this matrix decomposition, we first prepare $L$ and $U$ in `td.LULayer`, define the loss terms (`lower_tri_loss`, `upper_tri_loss`) to force the upper triangular part of $L$ and the lower triangular part of $U$ to take zero values as follows, and added to the matrix reconstruction error term in a weighted manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbe15f-db57-442f-9c7f-d77e71ad4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of LULayer\n",
    "lu_layer = td.LULayer(asym_mat)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(lu_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Weight for each objective function\n",
    "weights_lu = np.array([1, 2])\n",
    "weights_lu = weights_lu / sum(weights_lu)\n",
    "\n",
    "# Iteration\n",
    "loss_array_lu = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    x_pred = lu_layer()\n",
    "    # Loss\n",
    "    lower_tri_loss = torch.norm(lu_layer.L - torch.tril(lu_layer.L, diagonal=-1))\n",
    "    upper_tri_loss = torch.norm(lu_layer.U - torch.triu(lu_layer.U, diagonal=1))\n",
    "    loss = weights_lu[0] * torch.sum((asym_mat - x_pred)**2) + weights_lu[1] * (lower_tri_loss + upper_tri_loss)\n",
    "    loss_array_lu.append(loss.to('cpu').detach().numpy().copy())\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10de56a-5cdb-45a2-9c62-f16b4d732f91",
   "metadata": {},
   "source": [
    "#### Convergence Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a68a12-2455-43a1-8b87-ca6656c83aad",
   "metadata": {},
   "source": [
    "It is important to check the degree of loss reduction due to iterational calculations to see if the calculations are converging adequately. Especially in the gradient descent framework, the convergence is easily affected by the step size (`lu`) of `optim.SGD`, so if the values hardly change, are jagged, or even show an increase in loss, then tuning the step size parameter is essential. Tuning the weights parameter, which indicates which and how much loss to consider, is also equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2e0f6-64d7-4401-a275-041ca6417432",
   "metadata": {},
   "source": [
    "Here, the calculations appear to be well converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b68a65-8bd1-4aef-b699-2afe7c287137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_lu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0a4c0-a45c-4e71-9992-7a18974f11f0",
   "metadata": {},
   "source": [
    "#### Pattern Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b933453-f199-434f-b897-d6d189117047",
   "metadata": {},
   "source": [
    "Below are heat maps of $L$ and $U$ and their matrix products obtained by the above LU decomposition. It can be seen that both $L$ and $U$ are lower and upper triangular matrix-like, respectively, and the matrix product $LU$ seems a good approximation of the original matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db67f52-15de-4947-998e-1b831c39d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = lu_layer.L.detach().numpy()\n",
    "U = lu_layer.U.detach().numpy()\n",
    "LU = L @ U\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax1.pcolor(L)\n",
    "ax2.pcolor(U)\n",
    "ax3.pcolor(LU)\n",
    "ax4.pcolor(asym_mat)\n",
    "ax1.annotate('L', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax2.annotate('U', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax3.annotate('LU', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax4.annotate('X', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9e9e3-6018-4a78-b552-4627b4d9ffca",
   "metadata": {},
   "source": [
    "### QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df964b3a-d07e-4493-9ebb-4f9180e43994",
   "metadata": {},
   "source": [
    "In the QR decomposition, the asymmetric square matrix $X$ ($n \\times n$) is decomposed as the product of two factor matrices $Q$ and $R$ as follows.\n",
    "\n",
    "$$\n",
    "X = QR\n",
    "$$\n",
    ", where $Q$ is an $n \\times n$ orthogonal matrix ($Q^{T}Q=I$) and $R$ is an $n \\times n$ upper triangular matrix. QR decomposition is often used as a pre-processing step to make problems easier to solve, such as generalized inverse matrix or rbandomized SVD, by utilizing the properties of orthogonal matrix $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d58b16-af30-44f0-8ae1-6ff991b43193",
   "metadata": {},
   "source": [
    "As in the LU decomposition, the matrices $Q$ and $R$ are optimized iteratively as follows. Here, the error between $Q^{T}Q$ and $I$ is taken as loss (`ortho_loss`) to improve the orthogonality of $Q$. In addition, as with $U$ in the $LU$ decomposition, the lower triangular part of $R$ is taken as loss (`tri_loss`) to take zero values and added with the matrix reconstruction error with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d160542-4b8f-416b-94b1-6754290ebd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of LULayer\n",
    "qr_layer = td.QRLayer(asym_mat)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(qr_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Weight for each objective function\n",
    "weights_qr = np.array([3, 2, 1])\n",
    "weights_qr = weights_qr / sum(weights_qr)\n",
    "\n",
    "# Iteration\n",
    "loss_array_qr = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    x_pred = qr_layer()\n",
    "    # Loss\n",
    "    ortho_loss = torch.norm(torch.eye(qr_layer.Q.size(1)) - torch.mm(qr_layer.Q.t(), qr_layer.Q))\n",
    "    upper_tri_loss = torch.norm(qr_layer.R - torch.triu(qr_layer.R))\n",
    "    loss = weights_qr[0] * torch.sum((asym_mat - x_pred)**2) + weights_qr[1] * ortho_loss + weights_qr[2] * upper_tri_loss\n",
    "    loss_array_qr.append(loss.to('cpu').detach().numpy().copy())\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057aea20-fcbd-4c24-883b-4e49e045c7c8",
   "metadata": {},
   "source": [
    "#### Convergence Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c492d-fcc6-4157-9f87-58ec06042899",
   "metadata": {},
   "source": [
    "As in LU decomposition, the calculations appear to be well converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e69e8-41b7-43f6-96d0-a13fa7bc9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_qr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22833530-3046-4389-88cc-584219285bf1",
   "metadata": {},
   "source": [
    "#### Pattern Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532e880-8d9d-4bdf-8728-6234b7987295",
   "metadata": {},
   "source": [
    "We can see that $Q$ is a orthogonal-like matrix because the diagonal elements of $Q^{T}Q$ are large in value. $R$ is upper triangular matrix-like. The matrix product $QR$ seems a good approximation of the original matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49daaaf8-e218-49b7-91d4-a42fdfaa345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = qr_layer.Q.detach().numpy()\n",
    "R = qr_layer.R.detach().numpy()\n",
    "QR = Q @ R\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax1.pcolor(np.matmul(Q.T, Q))\n",
    "ax2.pcolor(Q)\n",
    "ax3.pcolor(R)\n",
    "ax4.pcolor(QR)\n",
    "ax5.pcolor(asym_mat)\n",
    "ax1.annotate('Q.T*Q', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax2.annotate('Q', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax3.annotate('R', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax4.annotate('QR', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax5.annotate('X', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30622b-8b20-4073-895a-c71d6027416c",
   "metadata": {},
   "source": [
    "## Symmetric Square Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a786a-5ea8-4d84-a3b0-9bd897c7d03b",
   "metadata": {},
   "source": [
    "Next, the Cholesky and eigenvalue decompositions are performed on `PyTorch`b as matrix decomposition algorithms for a symmetric square matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd38da-a691-4999-a55e-73334fef5497",
   "metadata": {},
   "source": [
    "### Toy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf5b6f-43d1-4d3c-87f6-46815a941104",
   "metadata": {},
   "source": [
    "Prepare the following artificial toy data. In order to allow each matrix decomposition algorithm to visually check the patterns contained in the data, rectangular blocks are placed in three locations and cross patterns in two locations, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49af13-599c-41fe-aaa7-173f94402627",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123456)\n",
    "sym_mat = torch.rand(30, 30, dtype=torch.float64)\n",
    "sym_mat = torch.mm(sym_mat, sym_mat.t())\n",
    "sym_mat[5:14, 7] = 20\n",
    "sym_mat[7, 5:14] = 20\n",
    "\n",
    "sym_mat[1:4, 25:29] = 20\n",
    "sym_mat[25:29, 1:4] = 20\n",
    "\n",
    "sym_mat[9:19, 15:16] = 20\n",
    "sym_mat[15:16, 9:19] = 20\n",
    "\n",
    "sym_mat[25:29, 25:29] = 20\n",
    "sym_mat[25:29, 25:29] = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolor(sym_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec801cc-fecb-4e08-8e80-38715f3a2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7b54d-7658-4756-948e-d1a9b01d703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_mat = sym_mat.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c4456-0681-443c-9d61-e6ba6591add5",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41194d2b-5787-46a1-853d-0532362086f0",
   "metadata": {},
   "source": [
    "The Cholesky decomposition is a special case of the LU decomposition when the input matrix $X$ is symmetric as follows:\n",
    "\n",
    "$$\n",
    "X = L^T L\n",
    "$$\n",
    "\n",
    ", where $L$ is an $n \\times n$ lower triangular matrix and $L$ has all diagonal elements 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed158c2d-f07f-43ff-a420-be2bde3204f4",
   "metadata": {},
   "source": [
    "As with the LU decomposition, we define the loss (`cholesky_loss`) to the upper triangular part of $L$ taking non-zero values and add it to the matrix reconstruction error term between $X$ and $LL^{T}$ with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15569f8-03bb-47b6-a270-fb8e0a2462c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of CholeskyLayer\n",
    "cholesky_layer = td.CholeskyLayer(sym_mat)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(cholesky_layer.parameters(), lr=1e-3)\n",
    "\n",
    "# Weight for each objective function\n",
    "weights_cholesky = np.array([10, 1])\n",
    "weights_cholesky = weights_cholesky / sum(weights_cholesky)\n",
    "\n",
    "# Iteration\n",
    "loss_array_cholesky = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    x_pred = cholesky_layer()\n",
    "    # Loss\n",
    "    cholesky_loss = torch.norm(cholesky_layer.L - torch.tril(cholesky_layer.L))\n",
    "    loss = weights_cholesky[0] * torch.sum((sym_mat - x_pred)**2) + weights_cholesky[1] * cholesky_loss\n",
    "    loss_array_cholesky.append(loss.to('cpu').detach().numpy().copy())\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc5c56-c9ca-40c0-a519-f669b2340352",
   "metadata": {},
   "source": [
    "#### Convergence Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20ddba-4c1a-4f58-9dc7-9d96cdf47ce5",
   "metadata": {},
   "source": [
    "The calculations appear to be well converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae2e43-94c5-4d95-9991-ca521d6fd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_cholesky)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003540a8-e777-43e7-ab95-faa9eb8c8664",
   "metadata": {},
   "source": [
    "#### Pattern Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571c030-dc75-4e26-99d2-5a19bedcf96f",
   "metadata": {},
   "source": [
    "The matrix product $LL^{T}$ seems a good approximation of the original matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89636ad-0b1c-4b16-9763-6a285ec95201",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = cholesky_layer.L.detach().numpy()\n",
    "LL = L @ L.T\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax1.pcolor(L)\n",
    "ax2.pcolor(LL)\n",
    "ax3.pcolor(sym_mat)\n",
    "ax1.annotate('L', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax2.annotate('L*L.T', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax3.annotate('X', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40d71c-e35e-497a-ac4c-001e1032d264",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition (EVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055946ba-8ac4-44aa-91ed-4555fd3c2816",
   "metadata": {},
   "source": [
    "Finally, we perform eigenvalue decomposition (EVD).\n",
    "\n",
    "EVD is a similar method to QR decomposition,\n",
    "\n",
    "$$\n",
    "X = Q \\Lambda Q^{T}\n",
    "$$\n",
    "\n",
    ", where $Q$ is an $n \\times n$ orthogonal matrix ($Q^{T}Q=I$) and $\\Lambda$ is a $n \\times n$ diagonal matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8c18d-0f28-468d-be8c-cb7a7e61fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of SymRecLayer\n",
    "n_components = 5\n",
    "evd_layer = td.SymRecLayer(sym_mat, n_components)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(evd_layer.parameters(), lr=0.0005)\n",
    "\n",
    "# Weight for each objective function\n",
    "weights_evd = np.array([1, 2, 3])\n",
    "weights_evd = weights_evd / sum(weights_evd)\n",
    "\n",
    "# Iteration\n",
    "loss_array_evd = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    x_pred = evd_layer()\n",
    "    # Loss\n",
    "    ortho_loss_Q = torch.norm(torch.eye(evd_layer.Q.size(1)) - torch.mm(evd_layer.Q.t(), evd_layer.Q))\n",
    "    diag_loss_Lambda = torch.norm(torch.eye(n_components) - evd_layer.Lambda)\n",
    "    loss = weights_evd[0] * torch.sum((sym_mat - x_pred)**2) + weights_evd[1] * ortho_loss_Q + weights_evd[2] * diag_loss_Lambda\n",
    "    loss_array_evd.append(loss.to('cpu').detach().numpy().copy())\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ed9ee-f54f-4bcb-8806-60c5b2a72bba",
   "metadata": {},
   "source": [
    "### Convergence Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595663c7-c0a2-4d34-aeb8-99e9b3471e64",
   "metadata": {},
   "source": [
    "The calculations appear to be well converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ea9eb-22ca-44c8-ade0-e0b61e6a6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array_evd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49728efc-4321-40de-a58e-99f928860e2e",
   "metadata": {},
   "source": [
    "### Patten Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59199b26-0044-4039-a905-779f8ca4fe68",
   "metadata": {},
   "source": [
    "The matrix product $Q \\Lambda Q^{T}$ seems a good approximation of the original matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2e900-f7ab-4d77-9bbe-49f1d08c77ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = evd_layer.Q.detach().numpy()\n",
    "Lambda = evd_layer.Lambda.detach().numpy()\n",
    "QLambdaQ = Q @ Lambda @ Q.T\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax1.pcolor(Q.T @ Q)\n",
    "ax2.pcolor(Q)\n",
    "ax3.pcolor(Lambda)\n",
    "ax4.pcolor(QLambdaQ)\n",
    "ax5.pcolor(sym_mat)\n",
    "ax1.annotate('Q.T*Q', xy=(0.5,4.3), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax2.annotate('Q', xy=(0.5,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax3.annotate('Lambda', xy=(0.5,4.3), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax4.annotate('Q*Lambda*Q', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "ax5.annotate('X', xy=(2,26), bbox=dict(boxstyle='round', fc='w'))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17af189-33b7-4b85-acd6-0ac5618a9391",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38b579-6d9c-4e54-8f45-c22645b328b5",
   "metadata": {},
   "source": [
    "- **LU/QR/Cholesky/Eigenvalue Decomposition**\n",
    "  - Gene H. Golub, Charles F. Van Loan Matrix Computations (Johns Hopkins Studies in the Mathematical Sciences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
